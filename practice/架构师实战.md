# 架构师实战


## dubbo

### 高性能NIO通讯及多协议集成

### 服务动态寻址与路由

### 软负载均衡与容错

### 依赖分析与降级等

### [安全](https://www.cnblogs.com/ghj1976/p/3778331.html)

- 服务黑白名单

- token令牌环

	- TokenFilter

	- 过Token令牌防止用户绕过注册中心直连，然后在注册中心上管理授权

### 路由规则

- 黑白名单

- 读写分离

### [分布式事务](http://javatar.iteye.com/blog/981787)

### 限流

- TpsLimitFilter

	- 计数器限流

	- tps

- ExecuteLimitFilter

	- executes

	- 限制并发次数

### 降级

- mock=fail:return+null 服务失败后返回null

### 超时

- TimeoutFilter

	- 超时直接异常

### 缓存

- CacheFilter

	- cache

		- lru

### 参数校验

- ValidationFilter

### 启动时检查

- 启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止spring启动

### 上下文信息&lt;br&gt;

- RpcContext

### 隐式传参

- 隐式参数&lt;br&gt;

	![隐式传参](http://dubbo.apache.org/docs/zh-cn/user/sources/images/context.png)  
	

### 异步调用

- async

- Future

### 参数回调

### 事件通知

### 本地存根/本地伪装

- 本地调用远程之前做一些操作

- stub/mock

### 环境划分

- 服务分组

	- group

- 服务分版本号

	- version

		- 开发版本

		- 生产版本等

### 集群容错

- Failover Cluster

	- 失败自动切换，当出现失败，重试其它服务器

	- &lt;dubbo:service cluster=&quot;failsafe&quot; /&gt;

	- 失败自动切换，当出现失败，重试其它服务器

- Failfast Cluster

	- 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。&lt;br&gt;

- Failsafe Cluster

	- 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。&lt;br&gt;

- Failback Cluster&lt;br&gt;

	- 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。&lt;br&gt;

- Forking Cluster

	- 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=&quot;2&quot; 来设置最大并行数

- Broadcast Cluster

	- 广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息&lt;br&gt;

### 负载均衡

- Random LoadBalance

- RoundRobin LoadBalance

	- 默认非权重，按顺序轮询

	- 按权重， 如 a a a b b c，则顺序是 a b c a b a

- LeastActive LoadBalance

	- 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差

	- 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。&lt;br&gt;

- ConsistentHash LoadBalance

	- 一致性哈希算法-TreeMap实现

	- 环-&gt;节点挂掉，选择下一个节点

### 线程模型

- Dispatcher

	- all

		- 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等

	- message

		- 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行

- ThreadPool

	- fixed

	- cache

	- limit

		- 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题

	- eager

		- 优先创建Worker线程池。在任务数量大于corePoolSize但是小于maximumPoolSize时，优先创建Worker来处理任务。当任务数量大于maximumPoolSize时，将任务放入阻塞队列中。阻塞队列充满时抛出RejectedExecutionException

### 动态代理

- stub

	- 消费端

- skeleton

	- 服务端

### dubbo服务启动流程

- 暴露本地服务

	- 自己调用自己的服务

- 暴露远程服务

	![暴露服务](https://upload-images.jianshu.io/upload_images/3397380-765c6461cf42ea93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)  
	

- 启动netty

- 注册zookeeper

- 订阅zookeeper

- 监听zookeeper

### 暴露服务

- 首先将服务的实现封装成一个Invoker，Invoker中封装了服务的实现类

- Invoker封装成Exporter，并缓存起来，缓存里使用Invoker的url作为key

- 服务端Server启动，监听端口。（请求来到时，根据请求信息生成key，到缓存查找Exporter，就找到了Invoker，就可以完成调用。）

### dubbo模块

![dubbo模块](http://dubbo.apache.org/docs/zh-cn/dev/sources/images/dubbo-modules.jpg)  


- dubbo-common

	- 公共模块

- dubbo-remoting

	- http/netty/mina等网络框架

- dubbo-rpc

	- 各种协议以及动态代理，如http/dubbo/hessian等

- dubbo-registry

	- 注册中心

- dubbo-config

	- 配置中心

- dubbo-monitor

	- 监控中心

- dubbo-container

	- 容器模块

### 缺省依赖

- javassist.jar

	- 动态代理采用javassist字节码生成技术

- spring-context.jar

	- 配置采用spring加载

- netty.jar

	- 网络通讯默认netty

### 架构层次

![架构层次](http://dubbo.apache.org/docs/zh-cn/dev/sources/images/dubbo-framework.jpg)  


- Config 配置层

	- ServiceConfig

	- ReferenceConfig

- Proxy 服务代理层

	- 服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton

	- ProxyFactory

- cluster 路由层

	- Invoker

	- Cluster

	- Directory

	- Router

	- LoadBalance

- monitor 监控层

- protocol 远程调用层

	- Invocation

	- Result

	- Protocol

	- Invoker

	- Exporter

- exchange 信息交换层

	- Request

	- Response

	- Exchanger

- transport 网络传输层

	- Message

	- Channel

	- Transporter

	- Client

	- Server

	- Codec

- serialize 数据序列化层

	- Serialization

### 架构节点

![dubbo 架构](https://camo.githubusercontent.com/660e543510891254fa0ca6138af3350458aa0582/687474703a2f2f647562626f2e6170616368652e6f72672f696d672f6172636869746563747572652e706e67)  


- Registry

- Consumer

- Provider

- Container

	- spring/log4j及自定义等

	- 优雅停机

		- addShutdownHook

		- contain.stop

- Monitor

## [Spring Cloud](https://springcloud.cc/spring-cloud-dalston.html#_features)

## docker

## [Disruptor](https://tech.meituan.com/disruptor.html)

### [伪共享&lt;br&gt;](http://ifeve.com/disruptor-cacheline-padding/)

- 共享

	- 寄存器

	- L1 L2 L3Cache

	- QPI 总线传输(between sockets, not drawn) 

	- 主存

- 同个缓存行，一个属性值失败，导致相邻的也失效。如head相邻tail，head失效，导致tail也失效

- 缓存行

	- 64位机器是64字节

	- 避免伪共享

	- 8个long属性，jdk1.7

### 内存屏障

- 确保特殊数据的执行顺序

- 影响一些数据的可见性

- Java：volatile

### RingBuffer

- 环形数组

- 2的幂，&amp;获取下标

- 消费者

	- 多个消费者读不需要加锁

	- 询问可以获取的序号，等待序号写满再消费

- 生产者

	- 两阶段提交：buffer申请下一个节点，写完commit提交

	- ClaimSequence分发序号，并记录那些序号已被分配

	- 按顺序提交，后分配的先到，则自旋等待

### 原子变量

- 如果是用锁

	- 乐观锁/悲观锁

	- 死锁

	- 耗时

- CAS

- volatile：head，tail，size

- disruptor

### [译文](http://ifeve.com/disruptor/)

## Netty

### 原理

- 一个高性能、异步事件驱动的NIO框架

	- TCP

	- UDP

- 对比

	- 阻塞IO

	- 非阻塞IO

	- 异步IO

- 概念

	- EventLoopGroup

		- 对应多个Channel

	- Bootstrap

		- 启动，串联各个组件

	- ChannelInitializer

		- ChannelPipeline（线程安全），类似Filter，责任链模式

			- Handler（非线程安全）

				- decoder

				- encode

				- handler

	- ChannelFuture

		- 异步处理

### [Reactor模型](https://crossoverjie.top/2018/07/04/netty/Netty(2)Thread-model/)

- 单线程模型

	- work(n)

- 多线程模型

	- boss(1) work(n)

- 主从多线程模型

	- boss(n) work(n)

###  [TCP 拆、粘包](https://crossoverjie.top/2018/08/03/netty/Netty(3)TCP-Sticky/)

- TCP收到一个报文的时候，将两个Msg合并了

- Msg分离成两个报文发送

- 原因

	- 一个MSS为1460

		- 每个以太网帧都有最小的大小64bytes最大不能超过1518byte

		- 刨去以太网帧的帧头14Bytes和帧尾CRC校验部分4Bytes，剩下1500，称之为MTU

		- MTU：1500-IP包头部(20)-TCP包头部(20)等于1460

	- 发送缓冲区，接收缓冲区

- 解决

	- 末尾加换行符

	- 808协议，消息头+消息主题，消息头指定消息的长度

	- netty

		- LineBasedFrameDecoder

		- DelimiterBasedFrameDecoder

		- FixedLengthFrameDecoder

### [零拷贝机制](https://www.cnblogs.com/xys1228/p/6088805.html)

- CompositeByteBuf

	- 将多个 ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了各个 ByteBuf 之间的拷贝.

- Wrap

	- 通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象

- FileRegion

	- 以直接将文件缓冲区的数据发送到目标 Channel, 避免了传统通过循环 write 方式导致的内存拷贝问题

- 系统层面的零拷贝

	- 避免用户态和内核态来回拷贝数据

	- Linux：mmap系统调用，将用户空间内存映射到内核空间，避免来回拷贝

###  [backlog参数](http://www.ideabuffer.cn/2018/02/22/TCP%E7%9A%84%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E4%B8%8Ebacklog%E5%8F%82%E6%95%B0/)

- 半连接队列

	- 保存 SYN_SENT 以及 SYN_RECV 的信息

- 全连接队列 Accept

	- 保存全连接状态的请求

	- 在使用listen函数时，内核会根据传入的backlog参数与系统参数somaxconn，取二者的较小值

### 心跳机制

- IdleStateHandler

## [秒杀系统架构](https://blog.52itstyle.com/archives/2853/)

![秒杀](https://blog.52itstyle.com/usr/uploads/2018/05/21329137.png)  


### 前端优化

- 静态页面

- CDN技术

### 网络优化

- BGP多线机房，减少延迟

### 后端优化

- Nginx最佳配置、Tomcat连接池优化、Redis/数据库配置优化、数据库连接池优化

### 后段实施

- 分流

- 限流

- 消息队列/异步

- 缓存

- 主备、高可用

- 秒杀业务不影响其他应用，即隔离

- 降级/熔断

- 监控！监控！

## Mysql

### 索引

- 物理划分

	- 聚集索引

		- 索引的键值逻辑顺序决定了表数据行的物理存储顺序

	- 非聚集索引

		- 非聚集索引也就是存储的键值逻辑连续，但是在表数据行物理存储顺序上不一定连续的索引

- 算法划分

	- 哈希

	- B+树

	- FullText全文索引

- 逻辑划分

	- 主键索引

	- 普通索引

	- 多级索引

		- [最左前缀](https://www.cnblogs.com/php-rearch/p/5034118.html)

### [存储引擎](https://www.cnblogs.com/zlcxbb/p/5757245.html)

- MyISAM

	- 索引

		- 主索引

			- 非聚集索引

				- 实现

					- 索引文件

					- 数据文件

			- 叶节点的data域存放的是数据记录的地址

		- 辅助索引

			- key可以重复

			- 非聚集索引

	- 事务/锁

		- 非事务安全型的

		- 表级锁

		- 适合执行大量的SELECT

		- 查询表的行数：select count(*) from table直接独处保存好的行数

- InnoDB

	- 索引

		- 主索引

			- 聚集索引

			- 数据文件本身就是索引文件，通过B+树组织

			- 必须要有主键

			- 使用自增字段作为主键则是一个很好的选择

		- 辅助索引

			- 数据域存主键id

			- 辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录

	- [事务/锁](https://blog.csdn.net/lc0817/article/details/52757194)

		- 事务安全型

		- 行级锁

		- 大量的INSERT或UPDATE

		- 查询表的行数：不同于MyISAM，需要扫描全表

		- MVCC多版本并发控制

### 事务

- ACID

	- [隔离性](https://blog.csdn.net/qq_33290787/article/details/51924963)

		- Read uncommit

		- Read commit

			- update

		- [Repeatable read](http://www.zsythink.net/archives/1233)

			- [幻读(不允许update但允许insert)](http://blog.sina.com.cn/s/blog_499740cb0100ugs7.html)

			- MVCC和next-key lock解决幻读问题

			- [MVCC](https://my.oschina.net/hebaodan/blog/910443)

				- 版本号

					- 创建版本号

						- 数据何时被创建

					- 删除版本号

						- 数据何时被删除

					- 操作

						- insert 将新插入的行的创建版本号设置为当前系统的版本号，删除版本号为空

						- delete 将该行的删除版本号设置为当前系统的版本号，并不把数据实际删除

						- update 转换成insert + delete。新拷贝这行数据，将新行insert同时设置创建版本号为当前版本号，将旧行的删除版本号设置为当前版本号

						- select

							- 该行的创建版本号小于等于当前版本号，用于保证在事务创建时或事务开始之前这行数据是存在的。&lt;br&gt;

							- 该行的删除版本号大于当前版本或者为空。用于保证在事务开始之前这行数据没有被删除

				- 缺点

					- 同一个时间点不同事务查询到的结果可能是不同的

					- 必须为每一行存储更多的数据，做更多的检查工作

				- 优点

					- 大多数查询不需要加锁

				- purge

					- innodb会开启一个后台线程执行清理工作，具体的规则是将删除版本号小于当前系统版本的行删除，这个过程叫做purge

				- MVCC没有解决幻读问题

					- mysql的快照读和当前读机制引起

					- 设表A有a,b两个属性，T1事务修改了同一行r1的a属性，T2事务如没有任何对r1进行修改，则读取到的是事务前的数据，但如果对r1修改了，比如修改了b属性，则能看到最新的值，即T2事务对b属性的修改和T1事务对a属性的修改

			- [next-key lock](https://dev.mysql.com/doc/refman/5.7/en/innodb-next-key-locking.html)

				- 行记录锁+记录前的gap锁

				- 唯一索引

					- 优化成Record Lock

				- 其他

					- 行记录锁

					- 记录前的gap锁

					- 对非唯一索引的下一个键值加上gap lock

				- 操作

					- locking read即 lock for update和lock for share mode

					- update

					- delete

				- 尽量使用等值操作来update

			- 可重复读和提交读

				- InnoDB提供了这样的机制，在默认的可重复读的隔离级别里，可以使用加锁读去查询最新的数据。

				- 快照读

					- SELECT * FROM t_bitfly;

					- 基于MVCC，不需要加锁，速度快

				- 加锁读

					- SELECT * FROM t_bitfly FOR UPDATE;

					- SELECT * FROM t_bitfly&nbsp;LOCK IN SHARE MODE;

		- Serializable

### [分库分表](https://3gods.com/database/MySQL-Sharding-Approaches.html)

- mycat

- sharding-jdbc

### [锁](https://blog.csdn.net/cug_jiang126com/article/details/50544728)

- 共享锁：select lock in share mode

	- 适用于两张表存在业务关系时的一致性要求

- 排它锁：select for update

	- for  update适用于操作同一张表时的一致性要求。

## Redis

### [原子性](https://segmentfault.com/q/1010000006636934/a-1020000006637267)

- 单个操作原子

- 批量操作Multi/Exec

### 数据结构

- string

	- jemalloc 最少会分配 32 字节的空间，如果字符串再稍微长一点，那就是 64 字节的空间

	- embstr 最大能容纳的字符串长度是44

	- 字符串在长度小于 1M 之前，扩容空间采用加倍策略，也就是保留 100% 的冗余空间。当长度超过 1M 之后，为了避免加倍后的冗余空间过大而导致浪费，每次扩容只会多分配 1M 大小的冗余空间

- hash

	- hashmap

- set

	- hashset

- list

	- ziplist

	- quicklist

	- 常用于异步队列

		- blpop/brpop

		- rpop/rpush + sleep

- zset

	- 跳跃表

	- 哈希表

### 过期策略

- 定时删除

	- key创建一个定时器，让定时器在key的过期时间到来时删除

		- 优点

			- 内存实时释放

		- 缺点

			- 耗费CPU时间

			- 定时器性能损耗严重

- 懒汉式删除

	- key过期的时候不删除，每次通过key获取值的时候去检查是否过期，若过期，则删除，返回null

		- 优点

			- CPU占用少

		- 缺点

			- 可能产生内存泄露

- 定期删除

	- 每隔一段时间执行一次删除过期key操作

- redis采用懒汉式删除+定期删除

### [分布式锁](https://blog.csdn.net/hupoling/article/details/53411190)

- lock

	- setnx + get + getset

	- set key value nx ex 5000

- unlock

	- # delifequals&lt;br&gt;if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then&lt;br&gt;    return redis.call(&quot;del&quot;,KEYS[1])&lt;br&gt;else&lt;br&gt;    return 0&lt;br&gt;end

- 可重入性

	- Thread Local+引用计数来实现

### keys命令

- 生产环境使用，容易阻塞

- 使用scan替代

- scan 0 match b* count 20

### redis设计

- 单线程模型

	- 1. 多路复用，非阻塞IO

	- 2. 存内存数据访问，保证低延迟

	- 3. 单线程避免了线程切换和竞态产生的消耗

	- 4. 单线程模型，如果单条命令执行占用大量时间，可能导致其他请求阻塞，这个对redis来讲是致命的

### 持久化

- 快照

	- COW(copy on write)实现持久化

	- fork一个子进程，由操作系统copy父进程的内存数据，然后子进程慢慢持久化

- 增量日志AOF

	- 定期对AOF重写，相同key合并

		- bgrewriteaof

	- 记录redis实例启动以来的所有指令序列

	- 先逻辑处理再存盘

		- 每次fsync

		- 永不fsync

		- 每个一秒fsync

		- 在数据安全性和性能之间做一个折中

- Redis 4.0 混合持久化&lt;br&gt;

	- rdb 文件 snapshot

	-  AOF增量日志

### 主从同步

- 最终一致

- 增量同步&lt;br&gt;

	- 环形数组

	- 同步指令流

	- 设置合适的buffer环形数组很重要

- 快照同步&lt;br&gt;

	- 在主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中

	- 将快照文件的内容全部传送到从节点

	- 执行一次全量加载，加载之前先要将当前内存的数据清空

	- 加载完毕后通知主节点继续进行增量同步

- 新增从节点

	- 先快照同步，再增量同步

- 无盘复制&lt;br&gt;

- Wait 指令

	- wait 指令可以让异步复制变身同步复制，确保系统的强一致性 (不严格)

## RocketMQ

### [节点](https://blog.csdn.net/javahongxi/article/details/72956608)

![rocketmq架构](https://upload-images.jianshu.io/upload_images/6332814-7885601f065a3556.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/975/format/webp)  


- Broker

	- Broker 接收来自生产者的消息，储存以及为消费者拉取消息的请求做好准备

	- 支持推和拉及主从结构的容错机制

	- Broker跟所有的Namesrv保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有topic信息

- NameSvr

	- 为producer 和 consumer 提供路由信息

	- Topic注册成功后，namesrv集群中就有Topic跟Broker的映射关系

- Producer

	- 先跟Namesrv集群中的其中一台建立长连接，并从Namesrv中获取当前发送的Topic存在哪些Broker上

	- 再跟对应的Broker建立长连接，直接向Broker发消息

- Consumer

	- 跟其中一台Namesrv建立长连接，获取当前订阅Topic存在哪些Broker上

	- 直接跟Broker建立连接通道，开始消费消息

### 高并发特点

- Broker

	- 消息顺序写

		- 所有Topic数据同时只会写一个文件，一个文件满1G，再写新文件，真正的顺序写盘，使得发消息TPS大幅提高

	- 消息随机读

		- RocketMQ尽可能让读命中系统pagecache，因为操作系统访问pagecache时，即使只访问1K的消息，系统也会提前预读出更多的数据，在下次读时就可能命中pagecache，减少IO操作

### 负载均衡

- Broker上存Topic信息，Topic由多个队列组成，队列会平均分散在多个Broker上

- 而Producer的发送机制保证消息尽量平均分布到所有队列中，最终效果就是所有消息都平均落在每个Broker上

- Broker-&gt;Topic-&gt;Queue

### 动态伸缩

- Topic维度

	- 假如一个Topic的消息量特别大，但集群水位压力还是很低，就可以扩大该Topic的队列数

- Broker维度

	- 如果集群水位很高了，需要扩容，直接加机器部署Broker就可以

### 高可用

- 同步刷盘

- 异步刷盘

### 消费

- 广播消费

	- 每个消费者消费Topic下的所有队列，即一个消息可以重复被多个消费者消费

- 集群消费

	- 一个topic可以由同一个ID下所有消费者分担消费，即一个消息只能由某个消费者消费

### Topic

- 指定topic，它的queue个数，它对应的cluster。然后系统自动建立这个cluster(多个master + 多个slave) 和你的topic之间的映射关系

- vs kafka： 指定topic，它的partition个数，它的master/slave配比，然后系统自动从所有机器中，为每个topic_partition分配1个master + 多个slave

### CommitLog消息存储

- [写入](https://blog.csdn.net/chunlongyu/article/details/54576649)

	- 同1台机器上面所有topic的所有queue的消息，存放在一个文件里面，从而避免了随机的磁盘写入

	- 所有消息都存在一个单一的CommitLog文件里面，然后有后台线程异步的同步到ConsumeQueue，再由Consumer进行消费

	- Producer对其“顺序写”

	- 步骤

		- 从mappedFileQueue获取到最后一个mappedFile

		- 如果mappedFile为空，则创建一个

		- 按照消息格式写入到mapperFile

		- 刷盘flush

		- 同步到Slave

	- mappedFile（内存映射文件）的链表

- 读取

	- Consumer对其“随机读”

		- 实现：内存映射文件

		- 读写时，根据offset定位到链表中，对应的MappedFile，进行读写

			- 过MappedFile，就很好的解决了大文件随机读的性能问题

	- ConsumeQueue消费端消费

		- 定长的结构，每1条记录固定的20个字节

		- Consumer消费消息的时候，要读2次：先读ConsumeQueue得到offset，再读CommitLog得到消息内容

- vs kafka

	- 每个topic_partition对应一个日志文件，Producer对该日志文件进行“顺序写”，Consumer对该文件进行“顺序读”

	- partition或者topic个数过多时，磁盘竞争，Kafka的性能急剧下

### Broker与Namesrv的心跳机制

- 30s心跳间隔

- 如果某个Broker在2分钟之内都没有心跳，则认为该Broker下线

### 概念

- Topic

- Message

- Tag

- Producer

	- Producer Group 

	-  Broker Master

- Consumer

	- Consumer Group

	-  Broker Master和Slave

### 集群模式

- 单master节点

- 多master节点

	- 节点宕机只会对订阅该 topic 的应用造成影响

	- 磁盘一般是RAID10保证可靠性

	- 单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会收到影响

- 多 master 多 slave 异步复制模式

	- salve只读不写，类似于mysql master-slave模式

	- 可能会有消息丢失的问题

	- Master宕机后，消费者仍然可以从Slave消费

- 多 master 多 slave 同步双写模式

	- 发送单个消息 RT 会略长，性能相比异步复制低10%左右

	- 保证数据不丢失

	- 目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能

### [master/slave主从节点选取](https://blog.csdn.net/chunlongyu/article/details/54018010)

- 不需要选举，Master/Slave的角色也是固定的，通过配置来设定。

- 当一个Master挂了之后，你可以写到其他Master上，但不会说一个Slave切换成Master

- queue可以放在多个master上

- 3个master，6个partition共9个节点

- vs kafka

	- kafka每一个partition对应一个master和多个slave

	- master和slave通过选举得出

	- 1个master和2个salve: 三个节点，master和slave可以共用，所以是3个节点

### [顺序消息](https://blog.csdn.net/earthhour/article/details/78323026)

- 对于同一个queue可以确保FIFO

- 把消息确保投递到同一条queue，比如用OrderId作哈希，Producer发布到的肯定是同一个队列

- 确保Consumer接收时也是有序的

	- MessageListenerOrderly

### [事务消息](https://www.jianshu.com/p/fc47ebaff6a4)

![事务消息](https://upload-images.jianshu.io/upload_images/6302559-c872bab5f5e48ccf.png)  


## Kafka

## MongoDB

### 集群

- Master-Slave

	- 主：可读可写，当数据有修改的时候，会将oplog同步到所有连接的salve上去

	- 从：只读不可写，自动从Master同步数据

	- Master-Slave不支持链式结构，即Slave只能产品能从Master同步

- Replica Set

	![ MongoDB架构图](https://upload-images.jianshu.io/upload_images/701806-c322a645ef052565.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/500/format/webp)  
	

	- 主节点（Primary)

		- 接收所有的写请求，然后把修改同步到所有Secondary

	- 副本节点（Secondary)

		- 与主节点保持同样的数据集。当主节点挂掉的时候，参与选主&lt;br&gt;

	- 仲裁者（Arbiter）

		- 不保有数据，不参与选主，只进行选主投票

## [ZooKeeper](https://juejin.im/post/5b037d5c518825426e024473)

### 分布式协同服务

- CP

### 数据模型

- 树形结构，类似于件系统的目录

- ZNode（1MB）

	- data

		- 数据信息

	- stat

		- 事务ID、版本号、时间戳、大小等

	- ACL

		- 访问权限

	- child

- 触发器Watch

	- 修改通知客户端

- 层级结构

	- /root/A/B/C

### 集群

- 架构

	- 一主多从

	- 单调一致性

		- 事务id和版本号

- [协同算法ZAB](http://blog.xiaohansong.com/2016/08/25/zab/)

	- 状态

		- Looking

		- Following

		- Leading

	- 最大ZXID

		- epoch

		- 计数

	- quorum

		- 超过半数节点的集合

	- 步骤

		- Leader election

			- 半数以上的投票，该节点将会成为准Leader

				- [Fast Leader Election](https://www.cnblogs.com/leesf456/p/6107600.html)

					- 问题

						- 没有一个仲裁者来确定leader

						- 每一个实例本地可能已经存在数据，不确定哪个实例上的数据是最新的

					- 起因

						- 整个系统重启或某部分节点(特殊是Leader)重启，重新选举leader

					- 状态

						- LOOKING

						- Following

						- Leading

						- OBSERVING

							- 观察者

					- 步骤

						![选举流程](http://codemacro.com/assets/res/zk/fle-flow.png)  
						

						- Looking: 所有Server加入集群时都会推荐自己为leader，然后将（leader id 、 zixd 、 epoch）作为广播信息，广播到集群中所有的服务器(Server)。然后等待集群中的服务器返回信息

						-  接受来自各个服务器的投票

							- 判断有效性

							- 是否是本轮投票？是否来自LOOKING状态的服务器？

						- 处理投票

							规则一：如果vote_zxid大于self_zxid，就认可当前收到的投票，并再次将该投票发送出去。 规则二：如果vote_zxid小于self_zxid，那么坚持自己的投票，不做任何变更。 规则三：如果vote_zxid等于self_zxid，那么就对比两者的SID，如果vote_sid大于self_sid，那么就认可当前收到的投票，并再次将该投票发送出去。 规则四：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么坚持自己的投票，不做任何变更  
							

							- 优先检查ZXID。ZXID比较大的服务器优先作为Leader

							- 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器

						- 统计投票

						- 改变服务器状态

							- Leader： Leading

							- Follower： Following

					- 实现

						![选举实现](https://images2015.cnblogs.com/blog/616953/201612/616953-20161206114702772-1120304539.png)  
						

						- sendqueue-&gt;WorkerReceiver

							- 选票接收器

						- recvqueue-&gt;WorkerSender

							- 选票发送器

				- 不需要等待所有节点的选票，能够尽早选出 leader

			- 投票当中包含自己的服务器ID和最新事务ID（ZXID）

		- Discovery

			- 防止Two Leader的出现

			- Leader选出最大的epoch+1返回给Follower

			- Follower返回各自最大的ZXID和历史事务日志给Leader，Leader得出最大的ZXID

		- Synchronization

			- 把Leader刚才收集得到的最新历史事务日志，同步给集群中所有的Follower

	- Broadcast(类2PC协议)

		- 客户端发出写入数据请求给任意Follower

		- Follower把写入数据请求转发给Leader

		- Leader采用二阶段提交方式，先发送Propose广播给Followe

		- Follower接到Propose消息，写入日志成功后，返回ACK消息给Leader

		- Leader接到半数以上ACK消息，返回成功给客户端，并且广播Commit请求给Follower

### 应用

- 分布式锁

- 服务注册和发现

- 配置中心

### 缺点

- master故障，选举节点的时间太长（30~120s），选举期间集群不可用

## Eureka

### 分布式服务发现

- AP

### Raft

### 优点

- 可以很好的应对因网络故障导致部分节点失去联系的情况

- 节点挂掉不会影响正常节点

