# 分布式基础


## 分布式概念

### 概念

- 模型

	- 节点

		- 对等

			- 各个节点没有主次之分

		- 分布

			- 在空间上随意分布，水平扩展

		- 并发

			- 并发访问共享资源

	- 通讯

		- 三态

			- 成功

			- 失败

			- 超时

				- 执行成功但是返回超时

				- 支持幂等性

	- 存储

	- 异常

- 副本

	- 副本一致性

		- 强一致性

		- 单调一致性

		- 会话一致性

		- 最终一致性

		- 弱一致性

- 指标

	- 性能

		- 吞吐量

		- 响应时间

		- 并发

			- QPS

	- 可用性

	- 可扩展性

		- 线性扩展性

	- 一致性

### [原理](https://blog.csdn.net/kangbin825/article/details/71006546?locationNum=7&fps=1)

- 理论

	- ACID

		- 原子性

			- 一个事务的所有操作要么全部成功，要么全部失败

		- 一致性

			- 在事务开始或结束的时候，数据库处于一致状态

		- 隔离性

			- 不同事务之间互不影响

		- 持久性

			- 一旦事务完成，不可回退

	- BASE

		- 基本可用

		- 软状态

			- 数据可以在一段时间内不一致

		- 最终一致

	- CAP

		- 一致性

			- 数据一致更新

		- 可用性

			- 好的响应性能

		- 分区容忍性

			- 可靠性

		- 三者只能保留其二

- 事务

	- 2PC

		- 准备阶段

			- 协调者向参与者发起指令，参与者评估自己的状态，如果参与者评估指令可以完成，参与者会写redo或者undo日志

			- 锁定资源，但是不提交

		- 提交阶段

			- 提交操作、释放资源

		- 问题

			- 阻塞

				- 占用的资源一直被锁定

			- 单点故障

				- 协调者

			- 脑裂

- 数据分布方式

	- 哈希方式（哈希表）(Mola &amp; Armor)

		- 可扩展性差

	- 按数据范围存储（B树）(HBase)

	- 按数据量分布(HDFS)

		- chunk

		- 元数据服务器

	- 一致性哈希(Dynamo)

		- 值域为一个封闭的环

		- 优化：引入虚节点

- 副本与数据分布

	- 以机器为副本

	- 数据段：segment，fragment，chunk，partition

- 本地化计算

- 副本控制协议

	- 中心化

		- 由一个中心节点协调副本数据的更新、维护副本之间的一致 性

	- primary-secondary协议

		- 外部节点将更新操作发给 primary 节点，primary确定顺序，同步给secondry节点，根据secondary的完成情况，确定更新是否成功并返回给外部节点

		- primary的确定

			- 由专门的元数据服务器决定

		- primary的切换

			- 可能造成停服

		- 数据同步

	- 去中心化的副本协议

		- Paxos

- Lease协议

	- 一致性问题

	- 解决脑裂

## 分布式一致算法

### [Raft](https://www.cnblogs.com/mindwind/p/5231986.html)

- [demo](http://thesecretlivesofdata.com/raft/)

- 节点

	- Leader

		- 选举

			- 票数相同（Split Vote），则再起一轮选举

			- 票数最多选中为Leader

		- heartbeat timeout

		- 定时同步heartbeat到Follower

	- Follower

		- 初始状态

		- election timeout

			- 150ms~300ms

			- 超时时间

		- election term

	- Candidate

		- Leader挂掉，超时了，则Follower变为Candidate

- [Log replication](https://www.jdon.com/artichect/raft.html)

	- Leader提交数据

	- Leader接收数据，并复制到Follower节点

		- uncommitted

	- Follower向Leader确认接收

	- Leader确认多数之后，Leader向Client确认接收，告诉Follower确认接收

		- committed

- 三个问题

	- Leader election

	- log replication

	- safety

		- 失败

		- 分区

### [Paxos](https://www.cnblogs.com/linbingdong/p/6253479.html)

- 节点

	- Proposer

		- 向Acceptor提交提案（proposal），提案中含有决议（value）

			- 向所有的Acceptors发送Prepare(N_A)请求

			- 如果收到Reject(N_H)信息，那么重新发送Prepare(N_H+1)

			- 如果收到Acceptors集合的任意一个Majority的Promise(N_A, V_A)回复，那么如果所有的V_A均为空，Proposer自由选取一个V_A’，回发Accept(N_A, V_A’)；否则回发Accept(N_A, V_i)

			- 如果收到Nack(N_H)，回到(1.1)过程，发送Prepare(N_H+1)

			- 如果收到任意一个Majority所有成员的Accepted信息（表明选举完成），向所有Proposers发送自身成为leader的消息

			- 向Learner发送value值

	- Accepter

		- 审批提案

			- 接收Prepare(N_A)，如果N_A&gt;N_H，那么回复Promise(N_A, V_A)，并置N_H=N_A；否则回复Reject(N_H)

			- 接收Accept(N_A, V_A)，如果N_A&lt;N_H，那么回复Nack(N_H)信息，否则设置this.V_A=V_A，并且回复Accepted信息&nbsp;

			- Promise(N_A, V_A)：向Proposer保证不再接受编号小于N_H的提案

	- Learner

		- 执行提案含有的决议

- [提案生成算法](https://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95)

	- Proposer

		- Prepare

			如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，Value为V的提案[N,V]。这里的V是所有的响应中编号最大的提案的Value。如果所有的响应中都没有提案，那 么此时V就可以由Proposer自己选择。 生成提案后，Proposer将该提案发送给半数以上的Acceptor集合，并期望这些Acceptor能接受该提案。我们称该请求为Accept请求。（注意：此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合）

		- Accept

			如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，Value为V的提案[N,V]。这里的V是所有的响应中编号最大的提案的Value。如果所有的响应中都没有提案，那 么此时V就可以由Proposer自己选择。 生成提案后，Proposer将该提案发送给半数以上的Acceptor集合，并期望这些Acceptor能接受该提案。我们称该请求为Accept请求。（注意：此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合）

	- Accepter

		- Promise

		- Reject

		- Nack

		- Accepted

- 约束

	- P1: 一个Acceptor必须接受它收到的第一个提案

	- 一个提案被选定需要被半数以上的Acceptor接受

	- 提案=提案编号 + value

	- P2: 如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v

	- P2a: 如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v

	- P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。&lt;br&gt;&lt;br&gt;

		P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个： S中每个Acceptor都没有接受过编号小于N的提案。 S中Acceptor接受过的最大编号的提案的value为V

	- [P2c： 对于提议(n,v)，acceptor的多数派S中，如果存在acceptor最近一次(即ID值最大)&lt;br&gt;接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值&lt;br&gt;满足P2c则P2b成立 (P2c =&gt; P2b =&gt; P2a =&gt; P2)](https://juejin.im/entry/585f61af8d6d810065ea644d)

		![选举](https://user-gold-cdn.xitu.io/2016/12/26/b76ca53bb6cb8126393874eb40002dbf?imageView2/0/w/1280/h/960/format/webp/ignore-error/1) 1. ID为2的提议最早提出，根据P2c其提议值可为任意值，这里假设为a 2. acceptor A/B/C/E 在之前的决议中没有接受(accept)任何提议，因而ID为5的提议的值也可以为任意值，这里假设为b 3. acceptor B/D/E，其中D曾接受ID为2的提议，根据P2c，该轮ID为14的提议的值必须与ID为2的提议的值相同，为a 4. acceptor A/C/D，其中D曾接受ID为2的提议、C曾接受ID为5的提议，相比之下ID 5较ID 2大，根据P2c，该轮ID为27的提议的值必须与ID为5的提议的值相同，为b；该轮决议被多数派acceptor接受，因此该轮决议得以确定 5. acceptor B/C/D，3个acceptor之前都接受过提议，相比之下C、D曾接受的ID 27的ID号最大，该轮ID为29的提议的值必须与ID为27的提议的值相同，为b

	- P1a：一个Acceptor只要尚未响应过任何编号大于N的Prepare请求，那么他就可以接受这个编号为N的提案

	- 保证只有一个提案被决议

	- 注意选定提案和接受提案的区别

	- 不断地进行“确定一个值”的过程、再为每个过程编上序号，就能得到具有全序关系(total order)的系列值，进而能应用在数据库副本存储等很多场景

- [基于消息传递且具有高度容错特性的一致性算法](https://www.cnblogs.com/linbingdong/p/6253479.html)

	- 阶段一：Prepare

		阶段一： (a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。 (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。

	- 阶段二： Accept

		(a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定。 (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。

- Paxos死循环问题

	- 通过选取主Proposer，就可以保证Paxos算法的活性。至此，我们得到一个既能保证安全性，又能保证活性的分布式一致性算法——Paxos算法。

- Base Paxos

	- 第一阶段：Prepare

	- 第二个阶段：Accept

	- 两轮RPC

### Multi Paxos

![multi paxos](https://user-gold-cdn.xitu.io/2017/10/13/27e1ebf7ec573fbd11e107ce797e41ee?imageslim)

- 唯一个Proposer，或者Leader

- Prepare+Accept合并

- 简化了角色，一轮RPC

- &nbsp;当前提案：{N, I, {Va, Vb, Vc}}， Propose+Accept

- 下一个提案：{N, I + 1, {Va, Vb, Vc}}，Accept

- Leader挂了，再选一个：{N + 1, I, {Va, Vb, Vc}}

### Zab

- 节点

	- Leader

	- Follower

- zxid

	- 高32：纪元（epoch）编号

	- 低32：消息编号

- 恢复（recovery）

	- 已经被处理的消息不能丢

		- 选举拥有 proposal 最大值（即 zxid 最大） 的节点作为新的 leader：由于所有提案被 COMMIT 之前必须有合法数量的 follower ACK，即必须有合法数量的服务器的事务日志上有该提案的 proposal，因此，只要有合法数量的节点正常工作，就必然有一个节点保存了所有被 COMMIT 消息的 proposal 状态。

		- 新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理

		- 新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都commited了所有的消息

	- 被丢弃的消息不能再次出现

		- 之前挂了的 leader 重新启动并注册成了 follower，他保留了被跳过消息的 proposal 状态，但与整个系统的状态是不一致的，需要将其删除

- 广播（boardcast）

	- Leader 接收到消息请求后，将消息赋予一个全局唯一的 64 位自增 id，叫做：zxid，通过 zxid 的大小比较即可实现因果有序这一特性

	- Leader 通过先进先出队列（通过 TCP 协议来实现，以此实现了全局有序这一特性）将带有 zxid 的消息作为一个提案（proposal）分发给所有 follower

	- 当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK

	- 当 leader 接收到超过一半以上的 ACKs 后，leader 就向所有 follower 发送 COMMIT 命令，同时会在本地执行该消息

	- 当 follower 收到消息的 COMMIT 命令时，就会执行该消息

- Leader选举

	- Fast Leader Election 

### 2PC

- 阶段1

	- coordinator发起一个提议，分别询问个participants是否接受

- 阶段2

	- coordinator根据participants的反馈，提交或终止事务。如果participants全部同意则提交，只要有一个participant不同意就终止。

- 非宕机，可以完全满足全认同、值合法、可结束，是解决一致性问题的一种协议

- 宕机下？

### [3PC](https://ivanzz1001.github.io/records/post/distribute-systems/2017/08/22/distribute-systems-theory-part1)

![3PC](https://ivanzz1001.github.io/records/assets/img/distribute/timg-3pc.png)

- 阶段1：coordinator或watchdog未收到宕机participant的vote，直接中止事务；宕机的participant恢复后，读取logging发现未发出赞成vote，自行中止该次事务

- 阶段2： coordinator未收到宕机participant的precommit ACK，但因为之前已经收到了宕机participant的赞成反馈（不然也不会进入到阶段2），coordinator进行commit； watchdog可以通过询问其他participant获得这些信息，过程同理；宕机的participant恢复后发现收到precommit且已经发出赞成vote，则自行commit该次事务

- 阶段3：即便coordinator或watchdog未收到宕机participant的commit ACK，也结束该次事务；宕机的participant恢复后发现收到commit或者precommit，也将自行commit该次事务

### Gossip

### [NWR](https://segmentfault.com/a/1190000014503967)

- N：有多少份数据副本

- W：一次成功的写操作至少有w份数据写入成功

- R：一次成功的读操作至少有R份数据读取成功

- 当W+R&gt;N的时候，读取操作和写入操作成功的数据一定会有交集，这样就可以保证一定能够读取到最新版本的更新数据，数据的强一致性得到了保证

### MVCC

### [理论](https://www.bilibili.com/video/av21667358/)

- 系统模型

	- 异步环境（asynchronous)下，节点宕机（fail-stop)

	- 异步环境（asynchronous)下，节点宕机恢复（fail-recover)、网络分化（network partition)

- FLP定理

	- 否定了同时满足safety和liveness的一致性协议的存在

- 主从同步（master/slave）

	- 写操作master等待，直到所有slave返回

	- 一个节点失败，master阻塞，导致整个集群不可用

	- 提升一致性、牺牲可用性

- 多数派

	- 写成功数超过一半

	- 读从一半以上节点读

	- 在并发情况下，无法保证正确性---顺序性

		- set 0 set 5

		- set 5 set 0

- 租约(lease)&lt;br&gt;

	- 每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约

	- 租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题

	- redis分布锁就是用的lease的原理

- 选举算法

	- Bully算法

- state machine replication

- 共识算法

## [分布式ID](https://blog.csdn.net/hl_java/article/details/78462283)

### UUID（太长）

### Flicker方案-mysql自增id

### [snowflake-机器时钟](https://www.cnblogs.com/haoxinyue/p/5208136.html)

### TDDL序列-mysql+读取可用批次内存生成

### redis

### MongoDB的ObjectId

### [美团Leaf](https://tech.meituan.com/MT_Leaf.html)

## [分布式哈希](https://www.cnblogs.com/hapjin/p/5760463.html)

### 普通哈希算法

- 不利于扩容

### [一致性哈希算法](http://www.cnblogs.com/hupengcool/p/3659016.html)

- 把桶的个数固定下来，如2n-1，并组织成环的形状

- 优点

	- 当添加新机器或者删除机器时，不会影响到全部数据的存储，而只是影响到这台机器上所存储的数据(落在这台机器所负责的环上的数据)

- 缺点

	- 可能导致分布不均匀

	- 扩容时存在“热点”问题

- 引入了虚拟节点的概念

## [幂等性](https://blog.csdn.net/wangyan9110/article/details/70953273)

### [方案](https://blog.csdn.net/wangyan9110/article/details/70953273)

- 全局唯一ID

- 去重表

- 插入或更新

	- 数据库支持

- 多版本控制&lt;br&gt;

- 状态机控

### 定义

- f(x)=f(f(x))

## 服务治理

### [服务发现](http://www.cnblogs.com/Zachary-Fan/p/service_manage_discovery.html)

- 注册

	- ZooKeeper

	- Consul

	- Eureka

- 订阅

- 变更下发

	- 如zookeeper，创建临时节点，然后watch节点的变化

- 统计上报

- 鉴权

- 扩展

	- 基于版本号的服务管理，可以用于灰度发布

	- 请求的复制回放，用于模拟真实的流量进行压测

	- 给请求打标签用于实时的在线压测

	- 更灵活的负载均衡和路由策略

	- 内置的熔断机制，避免整个分布式系统产生雪崩效应

### [服务路由](https://blog.csdn.net/xundh/article/details/59492750)

![dubbo服务治理](http://pic.yupoo.com/kazaff/EopVm33y/Rzbmx.jpg)

- 步骤

	- 消费者从注册中心获取服务器地址，缓存到本地

	- 服务器提供者变更后，由注册中心通知到服务器消费者，刷新路由缓存列表

	- 消费者通过路由算法获取一台服务器地址，进行请求访问

- 负载均衡算法

	- 一致性hash

	- Round-Robin

	- 随机

	- 平均时延加权

	- 粘滞连接 

- 路由扩展

	- 本地路由

		- injvm 模式

			- 优先调用JVM内的服务提供者

		- innative 模式

			- 服务端和消费端部署在同一台机器

	- 条件路由规则

		- 通过IP条件表达式进行黑白名单访问控制

		- 流量引导，只暴露部分服务提供者

		- 读写分离

		- 前后端分离

		- 灰度升级

### 服务调度

- [超时机制](https://www.cnblogs.com/ASPNET2008/p/7292472.html)

	- 消费端一直轮询ResponseFuture直至超时或者收到服务端的返回结果

- 重试机制

## 服务网关

### 特点

- 一个服务器，进入系统的唯一节点

### 要求

- 高可用

- 高性能

- 高可扩展性

### 应用

- 请求路由

	- 调用端调用其他服务的地址统一交给gateway来处理

- 服务注册

- 缓存

- 负载均衡

- 弹力设计

	- 熔断

	- 流量控制

	- 服务降级

- 安全

	- 授权

	- 监控

- 其他

	- API聚合

	- API编排

## 高可用

## [RPC原理](https://www.cnblogs.com/LBSer/p/4853234.html)

### 原理

- 调用方（Client）通过本地的 RPC 代理（Proxy）调用相应的接口

- 本地代理将 RPC 的服务名，方法名和参数等等信息转换成一个标准的 RPC Request 对象交给 RPC 框架

- RPC 框架采用 RPC 协议（RPC Protocol）将 RPC Request 对象序列化成二进制形式，然后通过 TCP 通道传递给服务提供方 （Server）

- 服务端（Server）收到二进制数据后，将它反序列化成 RPC Request 对象

- 服务端（Server）根据 RPC Request 中的信息找到本地对应的方法，传入参数执行，得到结果，并将结果封装成 RPC Response 交给 RPC 框架

- RPC 框架通过 RPC 协议（RPC Protocol）将 RPC Response 对象序列化成二进制形式，然后通过 TCP 通道传递给服务调用方（Client）

- 调用方（Client）收到二进制数据后，将它反序列化成 RPC Response 对象，并且将结果通过本地代理（Proxy）返回给业务代码

### 通讯层协议

- packet

	- header

		- 类型（请求、响应），版本号，编码，超时时间，请求id，payload length等

	- payload

- 大端序 VS 小端序

### 应用层协议

## 分布式限流

### [算法](https://juejin.im/entry/57cce5d379bc440063066d09)

- 漏桶算法

- 队列算法

	- FIFO

	- 优先级队列

- [令牌桶算法](http://blog.didispace.com/spring-boot-request-limit/)

	- nginx

	- 限制流量的流入速率，可以出来一定的突发流量

	- 按固定速率添加令牌，有令牌就能处理，令牌为0，则被限流

	- 用队列实现

	- Guava的RateLimiter实现令牌桶

- 计数器算法

	- 当一个请求来时，就做加一操作，当一个请求处理完后就做减一操作

	- 秒杀活动：限制抢购次数

	- redis watch

- 漏桶算法

	- 限制流量的流出速率

	- 漏斗用队列实现，请求过多，积压请求，队列满了则拒绝请求

	- TCP：请求过多会有一个sync backlog的队列来缓冲请求

### 动态算法

- 基于响应时间的动态限流

	- 限流的值很难设置成一个固定值

		- TCP的拥塞控制算法

	- 按响应时间排序，P99的值超过设定阈值，则自动限流

	- 蓄水池算法采样数据，并进行排序

### [目的](http://jinnianshilongnian.iteye.com/blog/2305117)

- 对并发访问/请求进行限速或者一个时间窗口内的的请求进行限速来保护系统

- 拒绝服务

- 排队或等待（比如秒杀、评论、下单）

- 降级

### 分布式限流

### 节流

## 降级技术

## 熔断技术

## 分布式缓存

### 概念

- 缓存雪崩

	- 缓存在同一时间大面积失效

		- 缓存失效时间随机分布

- 缓存穿透

	- 查询既不在缓存，又不在数据库中的数据

- 缓存预热

- 缓存算法

	- FIFO

	- LRU

### 缓存更新方案

- Cache Aside

	- 失效：先从缓存取数据，若无，则从数据库中去，并放到缓存

	- 命中：直接从缓存取数据

	- 更新：先把数据更新至数据库中，让缓存失效

- Read/Write through

	- 缓存作代理，同时更新数据库

- Read through

	- 缓存方自己加载数据

- Write through

	- 命中缓存时，更新缓存，再更新数据库

## 分布式锁

### [基于redis](https://juejin.im/entry/5a502ac2518825732b19a595)

- setnx(lockkey, 当前时间+过期超时时间) ，如果返回1，则获取锁成功；如果返回0则没有获取到锁

- get(lockkey)获取值oldExpireTime ，并将这个value值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取

- 计算newExpireTime=当前时间+过期超时时间，然后getset(lockkey, newExpireTime) 会返回当前lockkey的值currentExpireTime

- 判断currentExpireTime与oldExpireTime 是否相等，如果相等，说明当前getset设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试

### [基于redis2](http://www.importnew.com/27477.html#comments)

- 获得锁

	- SET resource_name my_random_value NX PX 30000&lt;br&gt;

- 释放锁

	- if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then&lt;br&gt;    return redis.call(&quot;del&quot;,KEYS[1])&lt;br&gt;else&lt;br&gt;    return 0&lt;br&gt;end

		- 为了避免client1阻塞，client2获得锁，client1恢复之后，把锁释放了

### [分布式redlock](http://ifeve.com/redis-lock/)

- 特性

	- 安全特性：互斥访问，即永远只有一个 client 能拿到锁

	- 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区

	- 容错性：只要大部分 Redis 节点存活就可以正常提供服务

- [算法](https://zhuanlan.zhihu.com/p/40915772)

	- [起 N个 master 节点，分布在不同的机房尽量保证可用性。](https://zhuanlan.zhihu.com/p/40915772)

	- 得到当前的时间，微秒单位

	- 尝试顺序地在 N个实例上申请锁，当然需要使用相同的 key 和 random value，这里一个 client 需要合理设置与 master 节点沟通的 timeout 大小，避免长时间阻塞在一个fail了的节点

	- 当 client 在大于等于N/2+1个 master 上成功申请到锁的时候，且它会计算申请锁消耗了多少时间，这部分消耗的时间采用获得锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了

	- 如果锁申请到了，那么锁真正的 lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间

	- 如果 client 申请锁失败了（获取到的锁&lt;=N/2），那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态

	- 释放锁？ 只需要在所有节点都释放锁就行，不管之前有没有在该节点获取锁成功

- [缺点](https://juejin.im/post/59f592c65188255f5c5142d2)

	- RedLock中，为了防止死锁，锁是具有过期时间的。client1阻塞超时，client2获得锁，client1恢复之后，再次执行代码

		- 请求时加上版本号，获得锁并提交时判断版本号

	- RedLock 是一个严重依赖系统时钟的分布式系统。如果时间不同步，可能导致两个client同时获得锁

	- 对于提升效率的场景下，RedLock 太重

	- 对于对正确性要求极高的场景下，RedLock 并不能保证正确性

### [基于zk](http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html)

- 1. 客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推

- [2. 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁](http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html)

	- 避免羊群效应

- 3. 执行业务代码

- 4. 完成业务流程后，删除对应的子节点释放锁

- 死锁问题：zk可以通过sessionTimeout来删除节点

### [基于mysql](https://www.jianshu.com/p/c2b4aa7a12f1)

- 基于乐观锁

	- 增加一个版本号

- 基于悲观锁

	- for update

	- 避免死锁

### 其他考虑的

- 考虑锁的可重入性

- 提供非阻塞形式的锁

- 高可用

- 可持久化

## 配置中心

### 静态配置

### 动态配置

- 例子

	- 日志级别

	- 降级开关

	- 活动开关

- 划分

	- 按运行环境划分

		- 生产环境、测试环境、预发布环境等

	- 按依赖划分

	- 按层次划分

		- IaaS、PaaS、SaaS

- 版本号管理

- 变更通知

### xxl-conf实现

![配置中心架构图](https://raw.githubusercontent.com/xuxueli/xxl-conf/master/doc/images/img_07.png)

### apollo实现

![apollo架构图有](https://github.com/ctripcorp/apollo/raw/master/doc/images/overall-architecture.png)

## [分布式事务](https://mp.weixin.qq.com/s/RDnf637MY0IVgv2NpNVByw)

### XA

- X/Open DTP 定义的交易中间件与数据库之间的接口规范

- 组件

	- TM

	- RM

	- AP

	- CRM

- 2PC的实现方案

	- 阻塞协议。服务在投票后需要等待协调器的决定，此时服务会阻塞并锁定资源

	- 最差时间复杂度高

	- 不适用于子事务声明周期较长 

### [2PC](http://duanple.blog.163.com/blog/static/70971767201311810939564/)

- 问题

- 角色

	- 协调者

		- 请求提交

		- 提交

		- 回滚

	- 参与者

		- 确认是否能提交

		- 提交

			- redo

		- 回滚

			- undo

- 阶段

	- Prepare

	- Commit

### [3PC](http://blog.jobbole.com/95632/)

- 相比2PC引入超时机制

- 三个阶段

	- CanCommit

	- preCommit

	- doCommit

### [TCC](http://wuwenliang.net/2017/03/25/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%B9%8B%E8%81%8A%E8%81%8ATCC/)

### [消息队列](https://blog.csdn.net/forearrow/article/details/47778497)

- 记录日志+补偿

	- 正向补偿

		- 重试

	- 反向补偿

		- 回滚

- [Rocketmq实现](https://mp.weixin.qq.com/s/rE6l2iWY2lGxDCcog7Muvw)

	1. 事务发起方首先发送 prepare 消息到 MQ。 2. 在发送 prepare 消息成功后执行本地事务。 3. 根据本地事务执行结果返回 commit 或者是 rollback。 4. 如果消息是 rollback，MQ 将删除该 prepare 消息不进行下发，如果是 commit 消息，MQ 将会把这个消息发送给 consumer 端。 5. 如果执行本地事务过程中，执行端挂掉，或者超时，MQ 将会不停的询问其同组的其他 producer 来获取状态。 6. Consumer 端的消费成功机制有 MQ 保证。 

### [Soga](https://mp.weixin.qq.com/s/HfLvNvbnE8DrM3n-DXgRQQ)

- 实现

	- 微服务间：最终一致

	- 微服务内：事务一致性

- 理论

	- 理论基础，其假设所有事件按照顺序推进，总能达到系统的最终一致性

	- 分别定义提交接口以及补偿接口，当某个事务分支失败时，调用其他的分支的补偿接口来进行回滚

### [解决方案](https://www.cnblogs.com/guyun/p/6251742.html)

## 调用链监控

### 故障处理

## 服务安全

### 黑白名单

## 负载均衡

### 概念

- Load Balance

- 通过某种负载分担算法，将外部请求尽量均匀的分配到对称结构的服务器组合的机器上

### [算法](https://www.cnblogs.com/xrq730/p/5154340.html)

- 轮询（Round Robin）

- 随机法

- 源地址哈希法

	- 保证了相同客户端IP地址将会被哈希到同一台后端服务器

	- 扩容或缩容时容易引起雪崩效应

- 加权轮询（Weight Round Robin）法

	- 1. 从权重中选出最大的，减去总权重&lt;br&gt;2. 再给每个权重加上自身权重初始值&lt;br&gt;3. 再次轮训，直到权重为0，确定序列

- 加权随机法

- 最小连接数

- 一致性哈希算法

### 应用

- [web负载均衡](http://lobert.iteye.com/blog/2159970)

	- http重定向

		- http location转移到子服务器

		- 主站点服务器压力过大

			- 下载业务

	- dns负载均衡

		- 域名映射多台服务器

		- 存在缓存，会出现一定的延迟

	- 反向代理负载均衡

		- 扮演中间人角色，即调度器

		- 调度器基于http

		- 调度器必须等待实际服务器的HTTP响应，并将它反馈给用户

		- 反向代理服务器存在一定的开销，如创建线程，与实际服务器进行TCP通讯，分析http头部等

	- IP负载均衡(LVS-NAT)

		- 在传输层修改数据包的目标地址为实际服务器地址

		- iptables

			- ipvsadm

		- 受限于NAT服务器的网络带宽

		- 性能是反向代理的2倍起

	- [直接路由(LVS-DR)](https://www.zhihu.com/question/22610352)

		- 在数据链路层修改目标MAC地址转发到实际服务器

		- 实际服务器和调度器在同一个网段

		- 从负载均衡调度器进，从实际服务器出，这样就不会受限于调度器的出口带宽了

		- 原理

			- 设一台调度器，两台实际服务器，则三台机的默认网关需要相同，最后再设置同样的ip别名（虚拟ip）

			- 还要防止实际服务器响应来自网络中针对IP别名的ARP广播

	- IP隧道(LVS-TUN)

		- 将调度器收到的IP数据包封装在一个新的IP数据包中，转交给实际服务器

		- 实际服务器的响应数据包可以直接到达用户端

		- 实际服务器可以和调度器不在同一个WAN网段

		- IP隧道技术支持

			- 一些CDN服务器就是基于IP隧道技术实现的

